<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ralgos</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <header class="site-header">ralgos</header>

  <div id="copyright">
    This is the webpage for project "Probability amplification for Randomized Algorithms" being taken by Nimitt{*} under the supervision of Prof. Clement Canonne{#} <br>
    <center>
      * Undergraduate Student, IIT Gandhinagar <br>
      # Sr. Lecturer, The University of Sydney
    </center>
  </div>

  <nav class="navbar">
    <a href="../index.html">Home</a>
    <a href="../proposal.html">Project Proposal</a>
    <a href="../literature.html">Literature Review</a>
    <a href="../progress.html">Findings</a>
    <a href="../ralgolib.html">Ralgos</a>
    <a href="../contact.html">Contact</a>
  </nav>

  <div class="container">
    <div class="display">
      <div class="researchpaper">
        <div class="title">Distributed Gaussian Mean Testing under Communication Constraints with Limited Randomness</div>

        <div class="section">Problem</div>

        We study the sample complexity for distributed gaussian mean testing under communication constraints with limited shared randomness among users. 

        <div class="definition" data-title="Gaussian Mean Testing (GMT)"></div>
        Define \((d, \ell, s, \varepsilon)\)-GMT as a distributed setting problem involving users and a central referee. Each user \(i\) samples an independent sample \( x_i \sim \mathcal{N}(\mu, I_d) \) where \(\mu \in \R^d\) is an unknown vector and communicates \(l\) bits to the central referee. Users can access a \(s\) bit random string. 
        
        The solution to problem is the protocol that enables central referee to determine which of the following two hypothesis is correct, with high probability,
        \[ H_0: \mu = 0 \quad \text{(Null Hypothesis)} \\
           H_1: \|\mu\|_2 \geq \varepsilon \quad \text{(Alternative Hypothesis)} \]

        for some  \( \varepsilon > 0 \).

        <div class="definition" data-title="Sample Complexity">
        Define sample complexity for the \((d, \ell, s, \varepsilon)\)-GMT problem as the minimum number of users required, \(n(d, \ell, s, \varepsilon) \), to solve the problem.
        </div>

        The cases \(s = 0\) and \(s = \infty\) correspond to private-coin and public-coin settings. Acharya et al. [1] derive upper and lower bounds for sample complexity in these two extremes. However, the intermediate regime, with shared randomness budget \(s\), remains largely unexplored. In this article, we show upper bound for the sample complexity in the case \(s = O(\log d)\).

        <div class="section">Protocol</div>

        In the public-coin protocol of [1], users apply a shared Haar random rotation to their sample, spreading the signal across all coordinates, requiring infinte randomness. This allows each user to send the signs of only the first \(\ell\) coordinates of the sample to the referee. We replicate this approach using the Subsampled Randomized Hadamard Transform (SRHT) as approximation to the Haar random rotation, which is computationally efficient and uses finite structured randomness.

        <div class="definition" data-title="Subsampled Randomized Hadamard Transform">
        Define \(S\) as \((d,\ell)\)- SRHT, if \( S_{\ell\times d} \in \mathbb{R}^{\ell \times d} \) and \( S = PHD \), where:
        <ul>
          <li>\(D_{d\times d}\) is a diagonal matrix with 4-wise independent Rademacher random variables</li>
          <li>\(H_{d \times d}\) is the normalized Hadamard matrix of size \(d \times d\)</li>
          <li>\(P_{\ell \times d}\) is a projection matrix that selects first \(\ell\) elements</li>
        </ul>
        </div>

        <div class="remark">
        The construction of a \((d, \ell)\)-SRHT requires sampling \(d\) 4-wise independent samples from the rademacher distribution, needs \(O(\log(d))\) random bits. 
        </div>

        <div class="section">Closeness of SRHT to Haar Random Rotation</div>
        <div class="lemma">
        Let \( \mu \in \R^d \) be a fixed vector with \( \|\mu\|_2 = \epsilon \). Let \(R\) be a \((d,\ell)\)-SRHT. Then, we have:
        \[
       E[(\|R\mu\|_2^2)^2] \lt \frac{\epsilon^4}{d^2}(l^2+2l)
        \]
        </div>
        <div class="proof">
          Let \(y = HDx\) where \(H = \pm \frac{1}{\sqrt{d}}\) and \(D_{ii} = \delta_i\) for \(1 \le i \le d \) where \(\delta_i \sim\) Rademacher.

          \begin{aligned}
          \E[y_i] = \sum_{j=1}{d}H_{ij} \delta_{jj} x_j = 0 \\
          \E[y_i^2] = \sum_{j=1}{d}H_{ij}^2 \delta_{jj}^2 x_j^2 = \frac{\epsilon^2}{d} 
          \end{aligned}

          
          Define \(Z = \sum_{i = 1}^{l} y_i^2\). Let \(a_j = H_{ij}x_j\), then \(y = \sum_{i=1}^{d}a_j \delta_j \).
          \[
          \E[Z^2] = \frac{\epsilon^2 l}{d}
          \]

          \begin{aligned}
          y_i^4 = \sum_{j_1,j_2,j_3,j_4} a_{j_1} a_{j_2} a_{j_3} a_{j_4} \delta_{j_1} \delta_{j_2} \delta_{j_3} \delta_{j_4} \\
          \E[y_i^4] = \sum_{j_1,j_2,j_3,j_4} a_{j_1} a_{j_2} a_{j_3} a_{j_4} \E[ \delta_{j_1} \delta_{j_2} \delta_{j_3} \delta_{j_4} ]
          \end{aligned}

          Because \(\delta_i, \delta_j\) are \(4\)-wise independent, only terms with pairwise matching remain. Note:
          \begin{aligned}
            \E[\delta_j^4] &= 1 \\
            \E[\delta_j^2 \delta_k^2] &= 1 \\
            \E[y_i^4] &= \sum_{j=1}^{d}a_j^4 + \sum_{j \lt k} 6a_j^2a_k^2 \\
                    &= \sum_{j=1}^{d}a_j^4 + 3((\sum_{j = 1}^{d} a_j^2)^2 - \sum_{j=1}^{d}a_j^4)\\
                    &= 3 (\sum_{j = 1}^{d} a_j^2)^2 - 2\sum_{j=1}^{d}a_j^4 \\
            \E[y_i^4] &\lt 3 (\sum_{j = 1}^{d} a_j^2)^2 \\
                    &\lt 3 \frac{\epsilon^4}{d^2}
          \end{aligned}
          Now, because \( \|y\|_2^2 = \|x\|_2^2\), \(Cov(y_i^2,y_j^2) \le 0\):

          \begin{aligned}
          \E[y_i^2y_j^2] &\le \E[y_i^2]\E[y_j^2] \\
                          &\le \frac{\epsilon^4}{d^2}
          \end{aligned}
          Thus:
          \begin{aligned}
          Z^2 &= \sum_{i=1}^{d}y_i^4 + \sum_{i \neq j}y_i^2 y_j^2 \\
          \E[Z^2] &= \sum_{i=1}^{l}\E[y_i^4] + \sum_{i \neq j}\E[y_i^2 y_j^2] \\
                   &\lt 3l \frac{\epsilon^4}{d^2} + l(l-1) \frac{\epsilon^4}{d^2} \\
                   &\lt (l^2+2l)\frac{\epsilon^4}{d^2}

          \end{aligned}
          
        </div>
        <div class="lemma" data-title="Flattening Lemma">
          Let \( \mu \in \R^d \) be a fixed vector with \( \|\mu\|_2 = \epsilon \). Let \(R\) be a \((d,\ell)\)-SRHT. Then, for \(l \ge 3\) we have:
          \[
          \Pr[\|R\mu\|_2^2] \gt 0.08 \frac{\epsilon^2 l}{d}] \gt 1/2
          \]
        </div>
        <div class="proof">
          Let \(Z = \|R\mu\|_2^2\). Using Lemma 1 and Paley-Zygmund Inequality:
          \begin{aligned}
            \Pr[ Z \gt \theta \frac{\epsilon^2 l}{d}] &\gt (1-\theta)^2 \frac{\E[Z]^2}{\E[Z^2]} \\
                                    &\gt (1-\theta)^2 \frac{l^2}{l^2 + 2l} 
          \end{aligned}
          Take \(\theta = 1 - \sqrt{\frac{l+2}{2l}}\). Then:
          \begin{aligned}
          \Pr[ Z \gt \theta \frac{\epsilon^2 l}{d}] \gt 1/2
          \end{aligned}
          For \(l \ge 3\):
          \[
          \Pr[ Z \gt 0.08 \frac{\epsilon^2 l}{d}] \gt 1/2
          \]
          
        </div>




        <div class="section">Sample Complexity</div>

        Using Flatenning Lemma and (Section 3.2, [1]), we have,
 
        <div class="theorem" data-title="Sample Complexity with Limited Randomness">
          For an \((d,\ell,O(\log d),\varepsilon)\)-GMT problem, the sample complexity follows the upper bound,
        \[
        \boxed{
        n = O\left( \frac{ d }{ \sqrt{\ell} \varepsilon^2} \right)
        }
  
        \]
        </div>


        <div class="section">References</div>
        [1] Acharya, J., Canonne, C.L. & Tyagi, H.. (2020). Distributed Signal Detection under Communication Constraints <i>Proceedings of Thirty Third Conference on Learning Theory</i>, in <i>Proceedings of Machine Learning Research</i> 125:41-63. <a href="https://proceedings.mlr.press/v125/acharya20b.html">Link</a>
        <br>

      </div>
    </div>
  </div>

  <footer class="site-footer">
    &copy; 2025 Nimitt. All rights reserved.
  </footer>
</body>
</html>
